# üè• Medical AI Assistant

A production-style **medical question‚Äëanswering system** built with:

- **FastAPI** backend
- **Celery + Redis** for asynchronous processing
- **RAG** (Retrieval-Augmented Generation) over a Kaggle medical Q&A dataset using **ChromaDB**
- Multiple LLM providers: **Ollama** (local, default), **OpenAI**, and **Google Gemini**
- A clean **Streamlit UI**
- Optional **Docker Compose** setup to run everything with one command

> This project is for **educational and informational** purposes only.  
> It does **not** provide medical advice, diagnosis, or treatment.

---

## ‚ú® Features

- Ask free‚Äëform medical questions in natural language.
- Choose between:
  - **Ollama** local models (default, e.g. `llama3.2:3b`)
  - **OpenAI** models (with API key)
  - **Google Gemini** models (with API key)
  - **Auto routing** (selects an available provider)
- Optional **RAG (knowledge base)**:
  - Loads a Kaggle medical chatbot dataset (Q/A + tags).
  - Uses `sentence-transformers` embeddings.
  - Indexes in **ChromaDB**.
  - Retrieves top‚ÄëK similar Q/A pairs as context for the LLM.
- Asynchronous architecture:
  - FastAPI enqueues queries as Celery tasks.
  - Redis as broker + result backend.
  - UI polls task status and shows progress.
- Observability:
  - `/health` for API, Redis, and Chroma status.
  - `/api/v1/metrics` for basic usage stats (total queries, success rate).

---

## üß± Architecture

```

                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ      Streamlit UI      ‚îÇ
                ‚îÇ   (frontend service)   ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ HTTP (REST)
                          ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ        FastAPI         ‚îÇ
                ‚îÇ  /api/v1/query         ‚îÇ
                ‚îÇ  /api/v1/task/{id}     ‚îÇ
                ‚îÇ  /health, /metrics     ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ Celery enqueue
                          ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    Redis        ‚îÇ        Celery          ‚îÇ
(broker +      ‚îÇ   query_task worker    ‚îÇ
results)      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
‚îÇ 1) Optional RAG retrieve via ChromaDB
‚îÇ 2) LLM generation via Ollama/OpenAI/Gemini
‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Answer + metadata    ‚îÇ
‚îÇ    stored in Redis     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

```

**RAG pipeline:**

- `MedicalDatasetLoader` ‚Üí loads `/app/data/train_data_chatbot.csv`.
- `VectorStore` ‚Üí builds embeddings (e.g. `all-MiniLM-L6-v2`) and stores in **ChromaDB**.
- `RAGRetriever` ‚Üí given a query, returns top‚ÄëK relevant Q&A entries.

---

## üìÇ Project Structure

```

medical-ai-assistant/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 \# FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ config.py               \# Settings (env-based)
‚îÇ   ‚îú‚îÄ‚îÄ models/                 \# Pydantic schemas
‚îÇ   ‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ celery_tasks.py     \# Celery task logic
‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset_loader.py   \# Loads + preprocesses medical dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py     \# ChromaDB + embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retriever.py        \# RAGRetriever
‚îÇ   ‚îú‚îÄ‚îÄ llm_providers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_provider.py  \# Local Ollama LLM client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py  \# OpenAI client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gemini_provider.py  \# Gemini client
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py           \# Provider selection / auto routing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_chain.py        \# RAG orchestration
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ logger.py           \# Structured logging
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py        \# Streamlit UI
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ train_data_chatbot.csv  \# Medical Q\&A dataset (not in repo)
‚îú‚îÄ‚îÄ celery_worker.py            \# Celery app entry
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ README.md

```

---

## üîß Requirements

Python dependencies are in `requirements.txt`, including:

- `fastapi`, `uvicorn`
- `celery`, `redis`
- `streamlit`
- `langchain`, `chromadb`, `sentence-transformers`
- `ollama`, `openai`, `google-generativeai`

External services:

- **Redis 7+**
- **Ollama** running on the host, with at least one model pulled (e.g. `llama3.2:3b`).

---

## ‚öôÔ∏è Environment Configuration

Create a `.env` file in the project root:

```

API_HOST=0.0.0.0
API_PORT=8000

DATASET_PATH=/app/data/train_data_chatbot.csv
CHROMA_DB_DIR=/app/chroma_db/medical_kb
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAG_TOP_K=3

REDIS_URL=redis://redis:6379/0

OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.2:3b

OPENAI_API_KEY=
GEMINI_API_KEY=

LOG_LEVEL=INFO
APP_ENV=development

```

Place your dataset at `data/train_data_chatbot.csv` so that inside the container it appears at `/app/data/train_data_chatbot.csv`.

---

## üê≥ Docker Setup

### Dockerfile

- Based on `python:3.11-slim`
- Installs dependencies from `requirements.txt`
- Copies the project into `/app`
- Default command: run FastAPI with Uvicorn

### docker-compose.yml

Defines four services:

- `redis` ‚Äì Redis broker/result backend.
- `api` ‚Äì FastAPI backend (`app.main:app`).
- `worker` ‚Äì Celery worker (`query_task`).
- `streamlit` ‚Äì Frontend UI (`frontend/streamlit_app.py`).

---

## üöÄ Running with Docker

Make sure you have:

- Docker and Docker Compose installed.
- Ollama running on the host and model pulled:

```

ollama serve
ollama pull llama3.2:3b

```

Then from the project root:

```

docker compose build
docker compose up

```

Services:

- FastAPI: http://localhost:8000
- Streamlit UI: http://localhost:8501
- Redis: `localhost:6379` (from the `redis` service)

To stop:

```

docker compose down

```

---

## üí¨ Usage

1. Open the UI: http://localhost:8501  
2. Enter a question, e.g.:

   > What are the symptoms of diabetes?

3. Choose configuration:
   - Model: **Ollama** (default) or another provider.
   - Toggle **Use knowledge base (RAG)** on/off.
4. Click **Get answer**.
5. The UI shows:
   - Answer text.
   - Model used, latency, tokens.
   - Retrieved knowledge entries (when RAG is enabled).

You can also call the REST API directly:

```

curl -X POST "http://localhost:8000/api/v1/query" ^
-H "Content-Type: application/json" ^
-d "{\"query\": \"What is diabetes?\", \"model_choice\": \"ollama\", \"use_rag\": true}"

```

---

## üîê Safety

- Outputs can be inaccurate, biased, or incomplete.
- Never use this system for real medical diagnosis or treatment.
- Always consult a licensed medical professional.

---

## üõ† Future Improvements

- Replace Streamlit with a React/Next.js frontend.
- Add chat history and authentication.
- Implement streaming responses from Ollama/OpenAI.
- Add admin views for dataset curation and analytics.
